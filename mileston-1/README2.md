# Milestone 1: Initial Validation with ConvBench

This directory contains the initial proof-of-concept implementation used in **Milestone 1** of the project. The code here was used to generate the figures and validate the frequency-aware compression approach shown in the final report.

## Purpose

This milestone demonstrates:
1. **Rendering pipeline**: Converting text conversations to images (Glyph-style)
2. **Image-level DCT compression**: Block DCT with frequency thresholding
3. **Visual analysis**: Frequency spectrum visualization and residual analysis

## Key Files

### Core Implementation
- `src/compression/block_dct_vq.py` - Block DCT compression (used for Method 1)
- `src/rendering/renderer.py` - Text-to-image rendering
- `src/data/conversations.py` - Sample conversation handling

### Configuration
- `configs/demo_config.yaml` - Rendering parameters (DPI, font, layout)
- `src/config.py` - Configuration loader

### Evaluation
- `src/eval/openai_eval.py` - ChatGPT-4o API evaluation (ConvBench)
- `src/eval/metrics.py` - Word recall and compression metrics

## Figures Generated for Report

The figures in the final report were generated using this code:

1. **Frequency Spectrum Comparison**: Shows original vs compressed DCT coefficients
   - Generated by: `block_dct_vq.py` → `_visualize_spectrum()`
   - Demonstrates frequency-domain sparsity exploitation

2. **Visual Comparison**: Original, mild compression, aggressive compression
   - Generated by: `compress()` method with different `keep_ratio` values
   - Shows quality vs compression trade-off

3. **Residual Visualization**: Difference between original and compressed
   - Generated by: Subtracting compressed from original image
   - Highlights information loss in high frequencies

## Differences from Final Implementation

**Milestone 1** (this directory):
- Used **ConvBench dataset** with **ChatGPT-4o** (black-box API)
- Focus on proof-of-concept and visualization
- Smaller scale testing (~50 samples)

**Final Implementation** (src/evaluation/):
- Uses **MRCR dataset** with **GLM-4V-9B** (local inference)
- Full-scale evaluation on 4× A100 GPUs
- Parallel processing (251 samples × 3 configurations)
- Integrated into production pipeline

## Running Milestone 1 Code

### Generate Compression Visualizations

```python
from src.compression.block_dct_vq import BlockDCTThresholdCompressor
from PIL import Image

# Initialize compressor
compressor = BlockDCTThresholdCompressor(block_size=14)

# Compress and visualize
compressed_img = compressor.compress(
    input_image="path/to/rendered_conversation.png",
    output_path="output/compressed.png",
    keep_ratio=0.5  # 50% coefficient retention
)

# This automatically generates:
# - output/compressed.png (compressed image)
# - output/compressed_spectrum.png (frequency visualization)
```

### Key Parameters

- `block_size=14`: 14×14 pixel blocks for DCT
- `keep_ratio=0.5`: Retain 50% of coefficients (2× compression)
- `keep_ratio=0.3`: Retain 30% of coefficients (aggressive)
- `keep_ratio=0.1`: Retain 10% of coefficients (extreme)

## Evolution to Final Method

The block DCT implementation here (`block_dct_vq.py`) evolved into:
- **Method 1** in final implementation (image-level compression)
- Same core algorithm, different integration point
- Used with MRCR benchmark and local GLM-4V inference

## ConvBench vs MRCR

**ConvBench** (Milestone 1):
- Multi-turn conversation evaluation
- Used ChatGPT-4o via API
- Validated compression approach

**MRCR** (Final):
- Multi-needle Retrieval in Conversational Records
- Local GLM-4V-9B inference
- More rigorous retrieval-based evaluation
- 2/4/8-needle difficulty levels

## Dependencies

Same as main project (`requirements.txt` in parent directory):
- `torch`, `torchvision`
- `PIL` (Pillow)
- `numpy`, `scipy`
- `matplotlib` (for visualizations)

## Notes

- This code represents the **proof-of-concept phase**
- Demonstrates feasibility of frequency-aware compression
- Figures from this code appear in the final report
- The approach was validated here before scaling to full implementation

---

For the complete, production-ready implementation, see:
- **Method 1**: `src/evaluation/local_inference.py`
- **Method 2**: `src/evaluation/local_inference_compressed.py`

