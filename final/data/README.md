# MRCR Dataset

**Multi-needle Retrieval in Conversational Records** - Long-context vision-language benchmark.

## Files

- `processed_{needle}needle_0-128k.json` - Raw conversation data with metadata
- `processed_{needle}needle_0-128k.jsonl` - Rendered data with image paths (generated by `final/scripts/render_images.py`)

## Dataset Structure

```json
{
  "unique_id": "2needle_0k_8k.parquet-0",
  "question": "Prepend {random_prefix} to the 2nd essay...",
  "answer": "{random_prefix}Title: Essay content...",
  "random_string_to_prepend": "{random_prefix}",
  "context": "user: Here are examples...\nAssistant: ...",
  "image_paths": ["/abs/path/to/rendered_images/{unique_id}/page_001.png"],
  "config": {"font-path": "..Verdana.ttf", "dpi": 72, ...}
}
```

## Configurations

| Needle | Token Range | Examples |
|--------|-------------|----------|
| 2-needle | 0-128k | 252 |
| 4-needle | 0-128k | 252 |
| 8-needle | 0-128k | 252 |

## Rendering

Images are rendered from conversation text using ReportLab:

```bash
python final/scripts/render_images.py --font-path Verdana.ttf
```

**Output:** `final/rendered_images/{unique_id}/page_*.png` (absolute paths stored in JSONL)

**Settings:** DPI=72, Font=Verdana, Block size=14×14 (DCT compression)

## Storage

- Raw JSON: ~2 MB each
- Rendered images: ~8 GB total (752 examples × ~10 MB avg)
- Compressed images (Method 1): ~4 GB (50% DCT coefficients)

